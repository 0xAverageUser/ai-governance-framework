---
doc-status: Draft
sequence: 14
type:
 - Integrity
title: Inadequate system alignment
external-refs:
  - owasp-llm-top-10:
    - "LLM07: Inadequate AI Alignment"
---

There is a specific goal you want to achieve when using an AI system in a successful way. It may be an overarching project goal, o a specific requirement for single queries to the AI system. 
This risk describes when the AI system behaves in a way that doesn't align with the intended goal.

In the more basic instance, it can be seen as the AI model clearly not providing useful information. This is related to [hallucinations described in TR-14](#TR-14). On a more subtle level, the model may be providing answers that seems to lead to a goal, 
but a different one than what it was put in place for. It may be giving suboptimal answers that seem good enough, but put the emphasis on a different specific topic that, when using the AI model on scale, will in turn have undesirable consequences.

When using a LLM in combination with RAG, it may provide suboptimal information when better one is available somewhere in the RAG database, or skew actions towards a global goal by a combination of chosen outputs that, while not being completely wrong,
may not provide as a whole the best outcome for the company. This as usual may be hard to measure because of the [non-deterministic nature](#TR-6) of AI output.

For example, a customer support chatbot may not be providing complete answers that would better benefit users, although it never hallucinates or gives false information. This could have short term benefit of reducing how customers are reimbursed for example,
because they do not know their full legal rights. This may in turn drive to lawsuits by dissatisfied customers, that could have huge monetary and reputational costs.
Or a trading bot could be suggesting good stock picks, but avoiding others in a way that puts all investment risk in a worse diversified position.

An convoluted example would be an LLM creating content that reads normal and gets published, but in fact encodes information that can be read with the proper algorithm (like "pick the first letter of each word, but more complex); 
and that would be detrimental to be exfiltrated from the company, so competitors can obtain it.

#### Severity

Imperfect alignment with goals is in a small amount may be inevitable by the nature of non-deterministic approach. As in many other threats, depending how much responsibility on outputs by the AI system and how little monitoring is put on it, the problem may be
aggravated.

Pre-trained foundational models come withIn the case of LLM, when training takes place there are several evaluations made on the models to check for alignment. Some of these are Reinforcement Learning from Human Feedback (RLHF), involving training models from human-generated data,
or Preference Fine-Tuning, using labeled data to fine-tune language models. 

But misalignment can happen later just with very specific queries not tested for, or adding fine-tuning, RAG, custom system prompt, compounding with multi-step chain of thought, or multiple agents.

Consider the criticality of the information the model would have access to (via RAG, user input, etc), and how the output would be consumed (internal users responsible for their decisions, automated systems with light monitored, 
or published to the general public).

#### Links

* Research
  * [Keeping LLMs Aligned After Fine-tuning: The Crucial Role of Prompt Templates](https://arxiv.org/abs/2402.18540)
  * [SoFA: Shielded On-the-fly Alignment via Priority Rule Following](https://arxiv.org/abs/2402.17358)