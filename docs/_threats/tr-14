---
doc-status: Draft
sequence: 14
type:
 - Integrity
title: Inadequate system alignment
external-refs:
  - owasp-llm-top-10:
    - "LLM07: Inadequate AI Alignment"
---

#### Outline

There is a specific goal you want to achieve when using an AI system in a successful way. It may be a overarching project goal, o a specific requirement for single queries to the AI system. 
This risk describes when the AI system behaves in a way that doesn't align with the intended goal.

In the more basic instance, it can be seen as the AI model clearly not providing useful information. This is related to hallucinations described in [TR-14](#TR-14). On a more subtle level, the model may be providing answers that seems to lead to a goal, 
but a different one that what it was put in place for. It may be giving suboptimal answers that seems good enough, but put the enphasis on a different specific topic that, when using the AI model on scale, will in turn have undesirable consequences.

When using a LLM in combination with RAG, it may provide suboptional information when better one is available somewhere in the RAG database, or squew actions towards a global goal by a combination of chosen outputs that, while not being completely wrong,
may not provide as a whole the best outcome for the company. This as usual may be hard to measure because of the non-deterministic nature of AI output.

For example, a customer support chatbot may not be providing complete answers that would better benefit users, although it never hallucinating or giving false information. This could have short term benefit of reducing how customers are reimbursed for example,
because they do not know they full legal rights. This may in turn drive to lawsuit by dissatisfied customers, that could have huge monetary and reputational costs.
Or a trading bot could be suggesting good stock picks, but avoiding others in a way that put all investment risk in a worse diversified position.

An convoluted example would be an LLM creating content that reads normal and gets published, but in fact encodes information that can be read with the proper algorithm (like "pick the first letter of each word, but more complex); 
and that would be detrimental to be exfiltrated from the company, so competitors can obtain it.

#### Severity

Imperfect alignment with goals is in a small amount may be inevitable by the nature of non-deterministic approach. As in many other threats, depending how much responsability on outputs by the AI system and how little monitoring is put on it, the problem may be
aggraviated.

Pre-trained foundational models come withIn the case of LLM, when training takes place there are several evaluations made on the models to check for alignment. Some of these are Reinforcement Learning from Human Feedback (RLHF), involving training models from human-generated data,
or Preference Fine-Tuning, using labeled data to fine-tune language models. 

But misalignment can happen later just with very specific queries not tested for, or adding fine-tunning, RAG, custom system prompt, compounding with multi-step chain of though, or multiple agents.

Consider the criticality of the information the model would have access to (via RAG, user input, etc), and how the output would be consumed (internal users responsible for their decisions, automated systems with light monitored, 
or published to the general public).

#### Further reading

* [TR-6 - Non-deterministic behaviour](#TR-6)
* [TR-4	- Hallucination](#TR-4)
* [CT-11 - Human feedback loop](#CT-11)
* [CT-13 - Provide citations](#CT-13)
