---
doc-status: Pre-Draft
sequence: 11
type:
  - Detective
mitigates:
  - tr-5
  - tr-6
  - tr-11
title: Human feedback loop
--- 

There are several aspects of human feedback loop which are crucial to implementation of GenAI in any real world usecase. Without a good feedback, solution won't be as effective as it can be. Also there won't be any scope to improve it over time. Following are some of the considerations one should keep in mind while designing feedback loop:

1. Feedback should be aligned with your KPI. One should define good set of KPI (Key Performance Index) and evaluation metrics for a solution for e.g. how many queries were answered by correct answers annotated by subject matter experts (SME). KPI based feedback can be both quantitative and qualitative.
2. Intended use of feedback data should be defined. The developers of the solution should take the feedback based upon the intended use of the data:
  i) Prompt finetuning: Analyse the feedback from the users and review them against the answers provide by LLM. Based upon the analysis find the gaps in purpose of the solution, based upon reponses in context of the user feedback provided. Tune to prompt to bridge those gaps. 
 ii) RAG document update: Analyze the low rated responses and look to content improvement opportunities wherever quality of the content is the root cause.
iii) Model/Data Drift: Feedback data should be able quantifiably detect the model drift or data drift due to changes in foundational model version or data quality over a period of time. 
 iv) Other future potential advanced uses i.e. Reinforcement Learning or Finetuning of the model itself.
Defining these goals of the data usage will make ensure the longterm viability of the solution.
3. User Experience: The team implementing the solution should survey their target audience on how open they are to provide feedback and whether the feedback will hamper the effectiveness of the solution in any way.
4. Wide vs Narrow Feedback: Sometimes user experience is very impoprtant to solution and the feedback can come in the way of user experince. In such scenarios, the team shoud consider created a smaller group tester SMEs within the users who can collaborate with development team to provide continuous elaborate feedback to improve the solution.
    
As stated earlier, there are several ways one can collect the feedback. The choices are very much dependent on the use case and intention of data usage. Following are the two major categories of feedback: 

1. Quantifiable Feedback: These are generally questions to the user which can be answered with categorical or numerical rating. For e.g. How you rate the repose of the chatbot on scale 1-5 with descripting of scaling parameters. Qauntifiable parameters can be potentiatlly used in conjuction with other metrics for KPI. 
2. Qaulitative Feedback: These are open eneded quations where the use is expected to write free form text as feedback. This allow user to provide data points not covered in Quantifiable feedback. One can use NLP or another LLM to analyze and derive insights from it.

Another aspect of Human Feeback which one should consider is related to the growing field of LLM-as-a-Judge (CT-15). When we use LLM-as-a-Judge, one should take the same Quantifiable and Qaulitative feedback from the LLM as they will do from humans. This will provide them an opportunity to compare feedback between machine and humans. Also it will allow to rate the effectiveness of LLM-as-a-Judge. One should consider doing Narrow feedback on atleast a sample of the LLM-as-a-Judge results. The sample size (in terms of %) and the methoidology is very use case dependent. For e.g. if the use case is very critical them one needs to verify higher number of samples. 
    
